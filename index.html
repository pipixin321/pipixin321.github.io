<!DOCTYPE html>
<html lang="en">


<!-- === Header Starts === -->
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Huaxin Zhang(HUST)'s homepage.">
  <meta name="keywords" content="Huaxin Zhang, Computer Vision, MLLM, Video Understanding">
  <meta name="author" content="Huaxin Zhang">

  <title>Huaxin Zhang (HUST)</title>

  <link rel="stylesheet" type="text/css" href="assets/style.css">
  <link rel="icon" type="image/png" href="assets/icon_resized.png">
  <link rel="apple-touch-icon" type="image/png" href="assets/icon_resized.png">
</head>
<!-- === Header Ends === -->


<body data-new-gr-c-s-check-loaded="14.1014.0" data-gr-ext-installed="">


<!-- === Homepage Starts === -->
<table width="980px" align="center" border="0">
<tbody>
<tr>


<td></td>  <!-- Leave one column blank on the left. -->
<td valign="top">


<!-- === Avatar Starts === -->
<br>
<table style="font-size: 12pt;" width="100%" border="0">
<tbody>
  <tr>
    <td width="75%">
      <p style="font-size: 20pt;">
        <b>Huaxin Zhang (HUST)</b>
      </p>
      <p style="text-align: justify;">
        Currently, I am a Master student of HUST (Huazhong University of Science and Technology), 
        supervised by <a href='https://scholar.google.com.hk/citations?user=4tku-lwAAAAJ&hl=zh-CN'>Prof. Changxin Gao</a> 
        and Prof. <a href='https://scholar.google.com.hk/citations?user=ky_ZowEAAAAJ&hl=zh-CN'>Prof. Nong Sang</a>. 
        Prior to this, I received my Bachelor's degree at Huazhong University of Science and Technology in 2022.

      </p>
      <p style="text-align: justify;">
        My primary research interests lie within the fields of <b>Deep Learning</b> and <b>Computer Vision</b>, current and previous focal areas include:
        <li><b>Multi-modal Large Language Models</b> </li>
        <li><b>Video: Action and event understanding</b>, more specifically, Temporal Action Localization & Video Anomaly Detection. </li>
      </p>
      <br>
      <a href="https://scholar.google.com.hk/citations?user=oyfu0pgAAAAJ&hl=zh-CN" target="_blank">Google Scholar</a>&nbsp;&nbsp;/&nbsp;
      <a class="icon" href="https://github.com/pipixin321" target="_blank">Github</a>&nbsp;&nbsp;/&nbsp;
      <a class="icon" href="zhanghuaxin@hust.edu.cn" target="_blank">Email</a>
    </td>
    <td width="10%"></td>
    <td width="40%">
      <!-- <img width="250" src="./assets/me.png"> -->
      <div class="circle-mask">
			<img src="./assets/me.jpg" alt="Profile Picture">
		</div>
    </td>
  </tr>
</tbody>
</table>
<!-- === Avatar Ends === -->


<!-- === News Starts === -->
<br>
<h2>News</h2>
<ul>
  <!-- <li><strong>[09/2024]</strong> We have 1 paper accepted to <a href="https://nips.cc/" target="_blank">NeurIPS 2024</a> (Vancouver, Canada).</li> -->
  <li><strong>[12/2024]</strong> We release <a href="https://arxiv.org/pdf/2412.06171" target="_blank">HolmesVAU</a>, including a large-scale multi-granularity benchmark for video anomaly understanding and an <b>Anomaly-focused Temporal Sampler (ATS)</b>  to enhance both efficiency and accuracy of MLLMs in processing long videos.</li>
  <li><strong>[10/2024]</strong> We release <a href="https://arxiv.org/pdf/2410.13733" target="_blank">Arcana</a>, which aims to enhance the visual perception capabilities of MLLMs from <b>instruction data</b> and <b>model structure</b> aspects.</li>
  <li><strong>[06/2024]</strong> We release <a href="https://arxiv.org/pdf/2406.12235" target="_blank">HolmesVAD</a>, a novel framework that leverages rich multimodal instructions to enable <b>comprehensive explanations</b> for video anomay detection.</li>
  <li><strong>[03/2024]</strong> We release <a href="https://arxiv.org/abs/2403.06154" target="_blank">GlanceVAD</a>, which <b>primarily</b> explores the <b>single-frame supervision</b> in the video anomaly detection domain.</li>
  <li><strong>[12/2023]</strong> We release <a href="https://arxiv.org/pdf/2308.12608" target="_blank">HR-Pro</a> for temporal action localization in untrimmed videos, which largely surpasses all previous point-supervised methods, and even outperforms several competitive fully-supervised methods.</li>
  <li><strong>[06/2022]</strong> We release <a href="https://arxiv.org/pdf/2207.10915" target="_blank">GAM-Net</a>, a Graph-based Armband Modeling Network for arm movement recognition.</li>
  <li><strong>[07/2022]</strong> We release <a href="https://arxiv.org/pdf/2206.09082" target="_blank">the first place winning solution</a> for temporal action detection task in CVPR-2022 AcitivityNet Challenge.</li>
</ul>
<!-- === News Ends === -->

<!-- === Publication Starts === -->
<br>
<h2>
  Selected Publications
</h2>

<table cellspacing="17">
<tbody>
<tr name="HolmesVAU"></tr>
<tr name="Arcana"></tr>
<tr name="HolmesVAD"></tr>
<tr name="GlanceVAD"></tr>
<tr name="HR-Pro"></tr>
<tr name="CPN"></tr>
<tr name="GAM-Net"></tr>


</tbody>
</table>
<!-- === Publication Ends === -->

<!-- === Experience Starts === -->
<br>
<h2>Experiences</h2>


<table cellspacing="17">
<tbody>
  <tr>
    <td width="8%">
      <img style="width: 100%; max-height: 100px; object-fit: cover;" src="./assets/institutions/baidu.png">
    </td>
    <td>
      <div class="institution">Baidu VIS</div>
      <div class="period">Jan. 2024 - Jun. 2024</div>
      <div class="position">Algorithm Intern</div>
    </td>
  </tr>

  <tr>
    <td width="8%">
      <img style="width: 100%; max-height: 100px; object-fit: cover;" src="./assets/institutions/HUST.png">
    </td>
    <td>
      <div class="institution">Huazhong University of Science and Technology (HUST)</div>
      <div class="period">Sep. 2022 - Jun. 2025</div>
      <div class="position">Master Degree </div>
    </td>
  </tr>

  <tr>
    <td width="8%">
      <img style="width: 100%; max-height: 100px; object-fit: cover;" src="./assets/institutions/HUST.png">
    </td>
    <td>
      <div class="institution">Huazhong University of Science and Technology (HUST)</div>
      <div class="period">Sep. 2018 - Jun. 2022</div>
      <div class="position">Bachelor Degree </div>
    </td>
  </tr>

</tbody>
</table>
<!-- === Experience Ends === -->


<!-- Visitor Traffic -->
<table
  style="width:auto; max-width:100%; margin-right:auto; margin-left:auto; border:0px; border-spacing:0px; border-collapse:separate;">
  <tbody>
    <tr>
      <td style="padding:20px;width:10%;vertical-align:middle">
        <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=C3nz9ALU2pPM9LThPPaClyMeXCMfJsdY7v7Kji88edc&cl=ffffff&w=a"></script>
        <!-- <a href="https://clustrmaps.com/site/1c32k"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=C3nz9ALU2pPM9LThPPaClyMeXCMfJsdY7v7Kji88edc&cl=ffffff" /></a> -->
      </td>
    </tr>
  </tbody>
</table>
<!-- Visitor Traffic Ends -->


</td>
</tr>
</tbody>
</table>
<!-- === Homepage Ends === -->

<!-- Rendering Projects-->
<script type="text/javascript" src="./assets/project_renderer.js"></script>



</body>
</html>


